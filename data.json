{
    "1.1 Master Node Configuration Files": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "1.1.1 Ensure that the API server pod specification file permissions are": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/manifests/kube-apiserver.yaml\n",
        "default": "By default, the kube-apiserver.yaml file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.1.2 Ensure that the API server pod specification file ownership is set to": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/manifests/kube-apiserver.yaml\n",
        "default": "By default, the kube-apiserver.yaml file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.1.3 Ensure that the controller manager pod specification file": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/manifests/kube-controller-manager.yaml\n",
        "default": "By default, the kube-controller-manager.yaml file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.1.4 Ensure that the controller manager pod specification file": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml\n",
        "default": "By default, kube-controller-manager.yaml file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager\n",
        "impact": "None\n"
    },
    "1.1.5 Ensure that the scheduler pod specification file permissions are set": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/manifests/kube-scheduler.yaml\n",
        "default": "By default, kube-scheduler.yaml file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-scheduler/\n",
        "impact": "None\n"
    },
    "1.1.6 Ensure that the scheduler pod specification file ownership is set to": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/manifests/kube-scheduler.yaml\n",
        "default": "By default, kube-scheduler.yaml file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-scheduler/\n",
        "impact": "None\n"
    },
    "1.1.7 Ensure that the etcd pod specification file permissions are set to": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %a /etc/kubernetes/manifests/etcd.yaml\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/manifests/etcd.yaml\n",
        "default": "By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640.\n",
        "ref": "1. https://coreos.com/etcd\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "None\n"
    },
    "1.1.8 Ensure that the etcd pod specification file ownership is set to": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/manifests/etcd.yaml\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/manifests/etcd.yaml\n",
        "default": "By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.\n",
        "ref": "1. https://coreos.com/etcd\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "None\n"
    },
    "1.1.9 Ensure that the Container Network Interface file permissions are": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %a <path/to/cni/files>\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 <path/to/cni/files>\n",
        "default": "NA\n",
        "ref": "1. https://kubernetes.io/docs/concepts/cluster-administration/networking/\n",
        "impact": "None\n"
    },
    "1.1.10 Ensure that the Container Network Interface file ownership is set": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G <path/to/cni/files>\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root <path/to/cni/files>\n",
        "default": "NA\n",
        "ref": "1. https://kubernetes.io/docs/concepts/cluster-administration/networking/\n",
        "impact": "None\n"
    },
    "1.1.11 Ensure that the etcd data directory permissions are set to 700 or": {
        "audit": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the below command:\nps -ef | grep etcd\nRun the below command (based on the etcd data directory found above). For example,\nstat -c %a /var/lib/etcd\nVerify that the permissions are 700 or more restrictive.\n",
        "remediation": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the below command:\nps -ef | grep etcd\nRun the below command (based on the etcd data directory found above). For example,\nchmod 700 /var/lib/etcd\n",
        "default": "By default, etcd data directory has permissions of 755.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "None\n"
    },
    "1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd": {
        "audit": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the below command:\nps -ef | grep etcd\nRun the below command (based on the etcd data directory found above). For example,\nstat -c %U:%G /var/lib/etcd\nVerify that the ownership is set to etcd:etcd.\n",
        "remediation": "On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the below command:\nps -ef | grep etcd\nRun the below command (based on the etcd data directory found above). For example,\nchown etcd:etcd /var/lib/etcd\n",
        "default": "By default, etcd data directory ownership is set to etcd:etcd.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "None\n"
    },
    "1.1.13 Ensure that the admin.conf file permissions are set to 644 or": {
        "audit": "Run the following command (based on the file location on your system) on the master\nnode. For example,\nstat -c %a /etc/kubernetes/admin.conf\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/admin.conf\n",
        "default": "By default, admin.conf has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\n",
        "impact": "None.\n"
    },
    "1.1.14 Ensure that the admin.conf file ownership is set to root:root": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/admin.conf\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/admin.conf\n",
        "default": "By default, admin.conf file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubeadm/\n",
        "impact": "None.\n"
    },
    "1.1.15 Ensure that the scheduler.conf file permissions are set to 644 or": {
        "audit": "Run the following command (based on the file location on your system) on the master\nnode. For example,\nstat -c %a /etc/kubernetes/scheduler.conf\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/scheduler.conf\n",
        "default": "By default, scheduler.conf has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\n",
        "impact": "None\n"
    },
    "1.1.16 Ensure that the scheduler.conf file ownership is set to root:root": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/scheduler.conf\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/scheduler.conf\n",
        "default": "By default, scheduler.conf file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubeadm/\n",
        "impact": "None\n"
    },
    "1.1.17 Ensure that the controller-manager.conf file permissions are set": {
        "audit": "Run the following command (based on the file location on your system) on the master\nnode. For example,\nstat -c %a /etc/kubernetes/controller-manager.conf\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod 644 /etc/kubernetes/controller-manager.conf\n",
        "default": "By default, controller-manager.conf has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n",
        "impact": "None\n"
    },
    "1.1.18 Ensure that the controller-manager.conf file ownership is set to": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nstat -c %U:%G /etc/kubernetes/controller-manager.conf\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown root:root /etc/kubernetes/controller-manager.conf\n",
        "default": "By default, controller-manager.conf file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n",
        "impact": "None\n"
    },
    "1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nls -laR /etc/kubernetes/pki/\nVerify that the ownership of all files and directories in this hierarchy is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchown -R root:root /etc/kubernetes/pki/\n",
        "default": "By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained\nwithin it, are set to be owned by the root user.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.1.20 Ensure that the Kubernetes PKI certificate file permissions are set": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nls -laR /etc/kubernetes/pki/*.crt\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod -R 644 /etc/kubernetes/pki/*.crt\n",
        "default": "By default, the certificates used by Kubernetes are set to have permissions of 644\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.1.21 Ensure that the Kubernetes PKI key file permissions are set to 600": {
        "audit": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nls -laR /etc/kubernetes/pki/*.key\nVerify that the permissions are 600.\n",
        "remediation": "Run the below command (based on the file location on your system) on the master node.\nFor example,\nchmod -R 600 /etc/kubernetes/pki/*.key\n",
        "default": "By default, the keys used by Kubernetes are set to have permissions of 600\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "None\n"
    },
    "1.2 API Server": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "1.2.1 Ensure that the --anonymous-auth argument is set to false": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --anonymous-auth argument is set to false.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the below parameter.\n--anonymous-auth=false\n",
        "default": "By default, anonymous access is enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests\n",
        "impact": "Anonymous requests will be rejected.\n"
    },
    "1.2.2 Ensure that the --basic-auth-file argument is not set (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --basic-auth-file argument does not exist.\n",
        "remediation": "Follow the documentation and configure alternate mechanisms for authentication. Then,\nedit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and remove the --basic-auth-file=<filename>\nparameter.\n",
        "default": "By default, basic authentication is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/authentication/#static-password-file\n",
        "impact": "You will have to configure and use alternate authentication mechanisms such as tokens and\ncertificates. Username and password for basic authentication could no longer be used.\n"
    },
    "1.2.3 Ensure that the --token-auth-file parameter is not set (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --token-auth-file argument does not exist.\n",
        "remediation": "Follow the documentation and configure alternate mechanisms for authentication. Then,\nedit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and remove the --token-auth-file=<filename>\nparameter.\n",
        "default": "By default, --token-auth-file argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/authentication/#static-token-file\n2. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "You will have to configure and use alternate authentication mechanisms such as\ncertificates. Static token based authentication could not be used.\n"
    },
    "1.2.4 Ensure that the --kubelet-https argument is set to true": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --kubelet-https argument either does not exist or is set to true.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and remove the --kubelet-https parameter.\n",
        "default": "By default, kubelet connections are over https.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/\n",
        "impact": "You require TLS to be configured on apiserver as well as kubelets.\n"
    },
    "1.2.5 Ensure that the --kubelet-client-certificate and --kubelet-client-key": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --kubelet-client-certificate and --kubelet-client-key arguments\nexist and they are set as appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the\napiserver and kubelets. Then, edit API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the\nkubelet client certificate and key parameters as below.\n--kubelet-client-certificate=<path/to/client-certificate-file>\n--kubelet-client-key=<path/to/client-key-file>\n",
        "default": "By default, certificate-based kubelet authentication is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/\n3. https://kubernetes.io/docs/concepts/cluster-administration/master-nodecommunication/#apiserver---kubelet\n",
        "impact": "You require TLS to be configured on apiserver as well as kubelets.\n"
    },
    "1.2.6 Ensure that the --kubelet-certificate-authority argument is set as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --kubelet-certificate-authority argument exists and is set as\nappropriate.\n",
        "remediation": "Follow the Kubernetes documentation and setup the TLS connection between the apiserver\nand kubelets. Then, edit the API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the -kubelet-certificate-authority parameter to the path to the cert file for the certificate\nauthority.\n--kubelet-certificate-authority=<ca-string>\n",
        "default": "By default, --kubelet-certificate-authority argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/\n3. https://kubernetes.io/docs/concepts/cluster-administration/master-nodecommunication/#apiserver---kubelet\n",
        "impact": "You require TLS to be configured on apiserver as well as kubelets.\n"
    },
    "1.2.7 Ensure that the --authorization-mode argument is not set to": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --authorization-mode argument exists and is not set to AlwaysAllow.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --authorization-mode parameter to\nvalues other than AlwaysAllow. One such example could be as below.\n--authorization-mode=RBAC\n",
        "default": "By default, AlwaysAllow is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/authorization/\n",
        "impact": "Only authorized requests will be served.\n"
    },
    "1.2.8 Ensure that the --authorization-mode argument includes Node": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --authorization-mode argument exists and is set to a value to include Node.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --authorization-mode parameter to a\nvalue that includes Node.\n--authorization-mode=Node,RBAC\n",
        "default": "By default, Node authorization is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/authorization/node/\n3. https://github.com/kubernetes/kubernetes/pull/46076\n4. https://acotten.com/post/kube17-security\n",
        "impact": "None\n"
    },
    "1.2.9 Ensure that the --authorization-mode argument includes RBAC": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --authorization-mode argument exists and is set to a value to include RBAC.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --authorization-mode parameter to a\nvalue that includes RBAC, for example:\n--authorization-mode=Node,RBAC\n",
        "default": "By default, RBAC authorization is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/\n",
        "impact": "When RBAC is enabled you will need to ensure that appropriate RBAC settings (including\nRoles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.\n"
    },
    "1.2.10 Ensure that the admission control plugin EventRateLimit is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --enable-admission-plugins argument is set to a value that includes\nEventRateLimit.\n",
        "remediation": "Follow the Kubernetes documentation and set the desired limits in a configuration file.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml and set the below parameters.\n--enable-admission-plugins=...,EventRateLimit,...\n--admission-control-config-file=<path/to/configuration/file>\n",
        "default": "By default, EventRateLimit is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit\n3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f\n8b93d0a746/contributors/designproposals/admission_control_event_rate_limit.md\n",
        "impact": "You need to carefully tune in limits as per your environment.\n"
    },
    "1.2.11 Ensure that the admission control plugin AlwaysAdmit is not set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that if the --enable-admission-plugins argument is set, its value does not include\nAlwaysAdmit.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and either remove the --enable-admission-plugins\nparameter, or set it to a value that does not include AlwaysAdmit.\n",
        "default": "AlwaysAdmit is not in the list of default admission plugins.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit\n",
        "impact": "Only requests explicitly allowed by the admissions control plugins would be served.\n"
    },
    "1.2.12 Ensure that the admission control plugin AlwaysPullImages is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --enable-admission-plugins argument is set to a value that includes\nAlwaysPullImages.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --enable-admission-plugins parameter\nto include AlwaysPullImages.\n--enable-admission-plugins=...,AlwaysPullImages,...\n",
        "default": "By default, AlwaysPullImages is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages\n",
        "impact": "Credentials would be required to pull the private images every time. Also, in trusted\nenvironments, this might increases load on network, registry, and decreases speed.\nThis setting could impact offline or isolated clusters, which have images pre-loaded and do\nnot have access to a registry to pull in-use images. This setting is not appropriate for\nclusters which use this configuration.\n"
    },
    "1.2.13 Ensure that the admission control plugin SecurityContextDeny is": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --enable-admission-plugins argument is set to a value that includes\nSecurityContextDeny, if PodSecurityPolicy is not included.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --enable-admission-plugins parameter\nto include SecurityContextDeny, unless PodSecurityPolicy is already in place.\n--enable-admission-plugins=...,SecurityContextDeny,...\n",
        "default": "By default, SecurityContextDeny is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny\n3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac\n",
        "impact": "This admission controller should only be used where Pod Security Policies cannot be used\non the cluster, as it can interact poorly with certain Pod Security Policies\n"
    },
    "1.2.14 Ensure that the admission control plugin ServiceAccount is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --disable-admission-plugins argument is set to a value that does not\nincludes ServiceAccount.\n",
        "remediation": "Follow the documentation and create ServiceAccount objects as per your environment.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and ensure that the --disable-admission-plugins\nparameter is set to a value that does not include ServiceAccount.\n",
        "default": "By default, ServiceAccount is set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount\n3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-serviceaccount/\n",
        "impact": "None.\n"
    },
    "1.2.15 Ensure that the admission control plugin NamespaceLifecycle is": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --disable-admission-plugins argument is set to a value that does not\ninclude NamespaceLifecycle.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --disable-admission-plugins parameter\nto ensure it does not include NamespaceLifecycle.\n",
        "default": "By default, NamespaceLifecycle is set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle\n",
        "impact": "None\n"
    },
    "1.2.16 Ensure that the admission control plugin PodSecurityPolicy is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --enable-admission-plugins argument is set to a value that includes\nPodSecurityPolicy.\n",
        "remediation": "Follow the documentation and create Pod Security Policy objects as per your environment.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --enable-admission-plugins parameter\nto a value that includes PodSecurityPolicy:\n--enable-admission-plugins=...,PodSecurityPolicy,...\nThen restart the API Server.\n",
        "default": "By default, PodSecurityPolicy is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/admin/admission-controllers/#podsecuritypolicy\n3. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n",
        "impact": "The policy objects must be created and granted before pod creation would be allowed.\n"
    },
    "1.2.17 Ensure that the admission control plugin NodeRestriction is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --enable-admission-plugins argument is set to a value that includes\nNodeRestriction.\n",
        "remediation": "Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --enable-admission-plugins parameter\nto a value that includes NodeRestriction.\n--enable-admission-plugins=...,NodeRestriction,...\n",
        "default": "By default, NodeRestriction is not set.\n",
        "ref": "1.\n2.\n3.\n4.\nhttps://kubernetes.io/docs/admin/kube-apiserver/\nhttps://kubernetes.io/docs/admin/admission-controllers/#noderestriction\nhttps://kubernetes.io/docs/admin/authorization/node/\nhttps://acotten.com/post/kube17-security\n",
        "impact": "None\n"
    },
    "1.2.18 Ensure that the --insecure-bind-address argument is not set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --insecure-bind-address argument does not exist.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and remove the --insecure-bind-address\nparameter.\n",
        "default": "By default, the insecure bind address is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "Connections to the API server will require valid authentication credentials.\n"
    },
    "1.2.19 Ensure that the --insecure-port argument is set to 0 (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --insecure-port argument is set to 0.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the below parameter.\n--insecure-port=0\n",
        "default": "By default, the insecure port is set to 8080.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "All components that use the API must connect via the secured port, authenticate\nthemselves, and be authorized to use the API.\nThis includes:\nâ¢\nâ¢\nâ¢\nâ¢\nkube-controller-manager\nkube-proxy\nkube-scheduler\nkubelets\n"
    },
    "1.2.20 Ensure that the --secure-port argument is not set to 0": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --secure-port argument is either not set or is set to an integer value\nbetween 1 and 65535.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and either remove the --secure-port parameter or\nset it to a different (non-zero) desired port.\n",
        "default": "By default, port 6443 is used as the secure port.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n",
        "impact": "You need to set the API Server up with the right TLS certificates.\n"
    },
    "1.2.21 Ensure that the --profiling argument is set to false (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --profiling argument is set to false.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the below parameter.\n--profiling=false\n",
        "default": "By default, profiling is enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi\nling.md\n",
        "impact": "Profiling information would not be available.\n"
    },
    "1.2.22 Ensure that the --audit-log-path argument is set (Automated)": {
        "audit": "records documenting the sequence of activities that have affected system by individual\nusers, administrators or other components of the system. Even though currently,\nKubernetes provides only basic audit capabilities, it should be enabled. You can enable it by\nsetting an appropriate audit log path.\nRun the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --audit-log-path argument is set as appropriate.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --audit-log-path parameter to a suitable\npath and file where you would like audit logs to be written, for example:\n--audit-log-path=/var/log/apiserver/audit.log\n",
        "default": "By default, auditing is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/concepts/cluster-administration/audit/\n3. https://github.com/kubernetes/features/issues/22\n",
        "impact": "None\n"
    },
    "1.2.23 Ensure that the --audit-log-maxage argument is set to 30 or as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --audit-log-maxage argument is set to 30 or as appropriate.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --audit-log-maxage parameter to 30 or\nas an appropriate number of days:\n--audit-log-maxage=30\n",
        "default": "By default, auditing is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/concepts/cluster-administration/audit/\n3. https://github.com/kubernetes/features/issues/22\n",
        "impact": "None\n"
    },
    "1.2.24 Ensure that the --audit-log-maxbackup argument is set to 10 or": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --audit-log-maxbackup argument is set to 10 or as appropriate.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --audit-log-maxbackup parameter to 10\nor to an appropriate value.\n--audit-log-maxbackup=10\n",
        "default": "By default, auditing is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/concepts/cluster-administration/audit/\n3. https://github.com/kubernetes/features/issues/22\n",
        "impact": "None\n"
    },
    "1.2.25 Ensure that the --audit-log-maxsize argument is set to 100 or as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --audit-log-maxsize argument is set to 100 or as appropriate.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --audit-log-maxsize parameter to an\nappropriate size in MB. For example, to set it as 100 MB:\n--audit-log-maxsize=100\n",
        "default": "By default, auditing is not enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://kubernetes.io/docs/concepts/cluster-administration/audit/\n3. https://github.com/kubernetes/features/issues/22\n",
        "impact": "None\n"
    },
    "1.2.26 Ensure that the --request-timeout argument is set as appropriate": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --request-timeout argument is either not set or set to an appropriate\nvalue.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml and set the below parameter as appropriate and if needed. For example,\n--request-timeout=300s\n",
        "default": "By default, --request-timeout is set to 60 seconds.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://github.com/kubernetes/kubernetes/pull/51415\n",
        "impact": "None\n"
    },
    "1.2.27 Ensure that the --service-account-lookup argument is set to true": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that if the --service-account-lookup argument exists it is set to true.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the below parameter.\n--service-account-lookup=true\nAlternatively, you can delete the --service-account-lookup parameter from this file so\nthat the default takes effect.\n",
        "default": "By default, --service-account-lookup argument is set to true.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://github.com/kubernetes/kubernetes/issues/24167\n3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use\n",
        "impact": "None\n"
    },
    "1.2.28 Ensure that the --service-account-key-file argument is set as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --service-account-key-file argument exists and is set as appropriate.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the --service-account-key-file parameter\nto the public key file for service accounts:\n--service-account-key-file=<filename>\n",
        "default": "By default, --service-account-key-file argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://github.com/kubernetes/kubernetes/issues/24167\n",
        "impact": "The corresponding private key must be provided to the controller manager. You would\nneed to securely maintain the key file and rotate the keys based on your organization's key\nrotation policy.\n"
    },
    "1.2.29 Ensure that the --etcd-certfile and --etcd-keyfile arguments are": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as\nappropriate.\n",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the\napiserver and etcd. Then, edit the API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd\ncertificate and key file parameters.\n--etcd-certfile=<path/to/client-certificate-file>\n--etcd-keyfile=<path/to/client-key-file>\n",
        "default": "By default, --etcd-certfile and --etcd-keyfile arguments are not set\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://coreos.com/etcd/docs/latest/op-guide/security.html\n",
        "impact": "TLS and client certificate authentication must be configured for etcd.\n"
    },
    "1.2.30 Ensure that the --tls-cert-file and --tls-private-key-file arguments": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --tls-cert-file and --tls-private-key-file arguments exist and they\nare set as appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection on the apiserver.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the TLS certificate and private key file\nparameters.\n--tls-cert-file=<path/to/tls-certificate-file>\n--tls-private-key-file=<path/to/tls-key-file>\n",
        "default": "By default, --tls-cert-file and --tls-private-key-file arguments are not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/\n3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide\n",
        "impact": "TLS and client certificate authentication must be configured for your Kubernetes cluster\ndeployment.\n"
    },
    "1.2.31 Ensure that the --client-ca-file argument is set as appropriate": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --client-ca-file argument exists and it is set as appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection on the apiserver.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the client certificate authority file.\n--client-ca-file=<path/to/client-ca-file>\n",
        "default": "By default, --client-ca-file argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/\n3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide\n",
        "impact": "TLS and client certificate authentication must be configured for your Kubernetes cluster\ndeployment.\n"
    },
    "1.2.32 Ensure that the --etcd-cafile argument is set as appropriate": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --etcd-cafile argument exists and it is set as appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and set up the TLS connection between the\napiserver and etcd. Then, edit the API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd\ncertificate authority file parameter.\n--etcd-cafile=<path/to/ca-file>\n",
        "default": "By default, --etcd-cafile is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-apiserver/\n2. https://coreos.com/etcd/docs/latest/op-guide/security.html\n",
        "impact": "TLS and client certificate authentication must be configured for etcd.\n"
    },
    "1.2.33 Ensure that the --encryption-provider-config argument is set as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --encryption-provider-config argument is set to a EncryptionConfig file.\nAdditionally, ensure that the EncryptionConfig file has all the desired resources covered\nespecially any secrets.\n",
        "remediation": "Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit\nthe API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the master node and set the --encryption-provider-config parameter to the path of\nthat file:\n--encryption-provider-config=</path/to/EncryptionConfig/File>\n",
        "default": "By default, --encryption-provider-config is not set.\n",
        "ref": "1.\n2.\n3.\n4.\nhttps://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/\nhttps://acotten.com/post/kube17-security\nhttps://kubernetes.io/docs/admin/kube-apiserver/\nhttps://github.com/kubernetes/features/issues/92\n",
        "impact": "None\n"
    },
    "1.2.34 Ensure that encryption providers are appropriately configured": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nGet the EncryptionConfig file set for --encryption-provider-config argument. Verify\nthat aescbc, kms or secretbox is set as the encryption provider for all the desired\nresources.\n",
        "remediation": "Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file,\nchoose aescbc, kms or secretbox as the encryption provider.\n",
        "default": "By default, no encryption provider is set.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/\n2. https://acotten.com/post/kube17-security\n3. https://kubernetes.io/docs/admin/kube-apiserver/\n4. https://github.com/kubernetes/features/issues/92\n5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers\n",
        "impact": "None\n"
    },
    "1.2.35 Ensure that the API Server only makes use of Strong": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-apiserver\nVerify that the --tls-cipher-suites argument is set as outlined in the remediation\nprocedure below.\n",
        "remediation": "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the master node and set the below parameter.\n--tls-ciphersuites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM\n_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM\n_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM\n_SHA384\n",
        "default": "By default the Kubernetes API server supports a wide range of TLS ciphers\n",
        "ref": "1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-BestPractices#23-use-secure-cipher-suites\nNotes:\nThe list chosen above should be fine for modern clients. It's essentially the list from the\nMozilla \"Modern cipher\" option with the ciphersuites supporting CBC mode removed, as\nCBC has traditionally had a lot of issues\n",
        "impact": "API server clients that cannot support modern cryptographic ciphers will not be able to\nmake connections to the API server.\n"
    },
    "1.3 Controller Manager": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "1.3.1 Ensure that the --terminated-pod-gc-threshold argument is set as": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --terminated-pod-gc-threshold argument is set as appropriate.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and set the --terminated-pod-gcthreshold to an appropriate threshold, for example:\n--terminated-pod-gc-threshold=10\n",
        "default": "By default, --terminated-pod-gc-threshold is set to 12500.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n2. https://github.com/kubernetes/kubernetes/issues/28484\n",
        "impact": "None\n"
    },
    "1.3.2 Ensure that the --profiling argument is set to false (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --profiling argument is set to false.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and set the below parameter.\n--profiling=false\n",
        "default": "By default, profiling is enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi\nling.md\n",
        "impact": "Profiling information would not be available.\n"
    },
    "1.3.3 Ensure that the --use-service-account-credentials argument is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --use-service-account-credentials argument is set to true.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node to set the below parameter.\n--use-service-account-credentials=true\n",
        "default": "By default, --use-service-account-credentials is set to false.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n2. https://kubernetes.io/docs/admin/service-accounts-admin/\n3. https://github.com/kubernetes/kubernetes/blob/release1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controllerroles.yaml\n4. https://github.com/kubernetes/kubernetes/blob/release1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-rolebindings.yaml\n5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles\n",
        "impact": "Whatever authorizer is configured for the cluster, it must grant sufficient permissions to\nthe service accounts to perform their intended tasks. When using the RBAC authorizer,\nthose roles are created and bound to the appropriate service accounts in the kube-system\nnamespace automatically with default roles and rolebindings that are auto-reconciled on\nstartup.\nIf using other authorization methods (ABAC, Webhook, etc), the cluster deployer is\nresponsible for granting appropriate permissions to the service accounts (the required\npermissions can be seen by inspecting the controller-roles.yaml and controller-rolebindings.yaml files for the RBAC roles.\n"
    },
    "1.3.4 Ensure that the --service-account-private-key-file argument is set": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --service-account-private-key-file argument is set as appropriate.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and set the --service-account-privatekey-file parameter to the private key file for service accounts.\n--service-account-private-key-file=<filename>\n",
        "default": "By default, --service-account-private-key-file it not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n",
        "impact": "You would need to securely maintain the key file and rotate the keys based on your\norganization's key rotation policy.\n"
    },
    "1.3.5 Ensure that the --root-ca-file argument is set as appropriate": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --root-ca-file argument exists and is set to a certificate bundle file\ncontaining the root certificate for the API server's serving certificate.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and set the --root-ca-file parameter to\nthe certificate bundle file`.\n--root-ca-file=<path/to/file>\n",
        "default": "By default, --root-ca-file is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-controller-manager/\n2. https://github.com/kubernetes/kubernetes/issues/11000\n",
        "impact": "You need to setup and maintain root certificate authority file.\n"
    },
    "1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that RotateKubeletServerCertificate argument exists and is set to true.\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and set the --feature-gates parameter to\ninclude RotateKubeletServerCertificate=true.\n--feature-gates=RotateKubeletServerCertificate=true\n",
        "default": "By default, RotateKubeletServerCertificate is set to \"true\" this recommendation verifies\nthat it has not been disabled.\n",
        "ref": "1.\n2.\n3.\n4.\nhttps://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller\nhttps://github.com/kubernetes/features/issues/267\nhttps://github.com/kubernetes/kubernetes/pull/45059\nhttps://kubernetes.io/docs/admin/kube-controller-manager/\n",
        "impact": "None\n"
    },
    "1.3.7 Ensure that the --bind-address argument is set to 127.0.0.1": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-controller-manager\nVerify that the --bind-address argument is set to 127.0.0.1\n",
        "remediation": "Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kubecontroller-manager.yaml on the master node and ensure the correct value for the -bind-address parameter\n",
        "default": "By default, the --bind-address parameter is set to 0.0.0.0\n",
        "ref": "1. https://kubernetes.io/docs/reference/command-line-tools-reference/kubecontroller-manager/\nNotes:\nAlthough the current Kubernetes documentation site says that --address is deprecated in\nfavour of --bind-address Kubeadm 1.11 still makes use of --address\n",
        "impact": "None\n"
    },
    "1.4 Scheduler": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "1.4.1 Ensure that the --profiling argument is set to false (Automated)": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-scheduler\nVerify that the --profiling argument is set to false.\n",
        "remediation": "Edit the Scheduler pod specification file /etc/kubernetes/manifests/kubescheduler.yaml file on the master node and set the below parameter.\n--profiling=false\n",
        "default": "By default, profiling is enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-scheduler/\n2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi\nling.md\n",
        "impact": "Profiling information would not be available.\n"
    },
    "1.4.2 Ensure that the --bind-address argument is set to 127.0.0.1": {
        "audit": "Run the following command on the master node:\nps -ef | grep kube-scheduler\nVerify that the --bind-address argument is set to 127.0.0.1\n",
        "remediation": "Edit the Scheduler pod specification file /etc/kubernetes/manifests/kubescheduler.yaml on the master node and ensure the correct value for the --bind-address\nparameter\n",
        "default": "By default, the --bind-address parameter is set to 0.0.0.0\n",
        "ref": "1. https://kubernetes.io/docs/reference/command-line-tools-reference/kubescheduler/\n",
        "impact": "None\n"
    },
    "2.1 Ensure that the --cert-file and --key-file arguments are set as": {
        "audit": "Run the following command on the etcd server node\nps -ef | grep etcd\nVerify that the --cert-file and the --key-file arguments are set as appropriate.\n",
        "remediation": "Follow the etcd service documentation and configure TLS encryption.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the\nmaster node and set the below parameters.\n--cert-file=</path/to/ca-file>\n--key-file=</path/to/key-file>\n",
        "default": "By default, TLS encryption is not set.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "Client connections only over TLS would be served.\n"
    },
    "2.2 Ensure that the --client-cert-auth argument is set to true": {
        "audit": "Run the following command on the etcd server node:\nps -ef | grep etcd\nVerify that the --client-cert-auth argument is set to true.\n",
        "remediation": "Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master\nnode and set the below parameter.\n--client-cert-auth=\"true\"\n",
        "default": "By default, the etcd service can be queried by unauthenticated clients.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth\n",
        "impact": "All clients attempting to access the etcd server will require a valid client certificate.\n"
    },
    "2.3 Ensure that the --auto-tls argument is not set to true (Automated)": {
        "audit": "Run the following command on the etcd server node:\nps -ef | grep etcd\nVerify that if the --auto-tls argument exists, it is not set to true.\n",
        "remediation": "Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master\nnode and either remove the --auto-tls parameter or set it to false.\n--auto-tls=false\n",
        "default": "By default, --auto-tls is set to false.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls\n",
        "impact": "Clients will not be able to use self-signed certificates for TLS.\n"
    },
    "2.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set": {
        "audit": "Run the following command on the etcd server node:\nps -ef | grep etcd\nVerify that the --peer-cert-file and --peer-key-file arguments are set as appropriate.\nNote: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\n",
        "remediation": "Follow the etcd service documentation and configure peer TLS encryption as appropriate\nfor your etcd cluster.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the\nmaster node and set the below parameters.\n--peer-client-file=</path/to/peer-cert-file>\n--peer-key-file=</path/to/peer-key-file>\n",
        "default": "Note: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\nBy default, peer communication over TLS is not configured.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n",
        "impact": "etcd cluster peers would need to set up TLS for their communication.\n"
    },
    "2.5 Ensure that the --peer-client-cert-auth argument is set to true": {
        "audit": "Run the following command on the etcd server node:\nps -ef | grep etcd\nVerify that the --peer-client-cert-auth argument is set to true.\nNote: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\n",
        "remediation": "Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master\nnode and set the below parameter.\n--peer-client-cert-auth=true\n",
        "default": "Note: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\nBy default, --peer-client-cert-auth argument is set to false.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-clientcert-auth\n",
        "impact": "All peers attempting to communicate with the etcd server will require a valid client\ncertificate for authentication.\n"
    },
    "2.6 Ensure that the --peer-auto-tls argument is not set to true": {
        "audit": "Run the following command on the etcd server node:\nps -ef | grep etcd\nVerify that if the --peer-auto-tls argument exists, it is not set to true.\nNote: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\n",
        "remediation": "Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master\nnode and either remove the --peer-auto-tls parameter or set it to false.\n--peer-auto-tls=false\n",
        "default": "Note: This recommendation is applicable only for etcd clusters. If you are using only one\netcd server in your environment then this recommendation is not applicable.\nBy default, --peer-auto-tls argument is set to false.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n2. https://kubernetes.io/docs/admin/etcd/\n3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls\n",
        "impact": "All peers attempting to communicate with the etcd server will require a valid client\ncertificate for authentication.\n"
    },
    "2.7 Ensure that a unique Certificate Authority is used for etcd (Manual)": {
        "audit": "Review the CA used by the etcd environment and ensure that it does not match the CA\ncertificate file used for the management of the overall Kubernetes cluster.\nRun the following command on the master node:\nps -ef | grep etcd\nNote the file referenced by the --trusted-ca-file argument.\nRun the following command on the master node:\nps -ef | grep apiserver\nVerify that the file referenced by the --client-ca-file for apiserver is different from the -trusted-ca-file used by etcd.\n",
        "remediation": "Follow the etcd documentation and create a dedicated certificate authority setup for the\netcd service.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the\nmaster node and set the below parameter.\n--trusted-ca-file=</path/to/ca-file>\n",
        "default": "By default, no etcd certificate is created and used.\n",
        "ref": "1. https://coreos.com/etcd/docs/latest/op-guide/security.html\n",
        "impact": "Additional management of the certificates and keys for the dedicated certificate authority\nwill be required.\n"
    },
    "3.1 Authentication and Authorization": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "3.1.1 Client certificate authentication should not be used for users": {
        "audit": "Review user access to the cluster and ensure that users are not making use of Kubernetes\nclient certificate authentication.\n",
        "remediation": "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be\nimplemented in place of client certificates.\n",
        "default": "Client certificate authentication is enabled by default.\nNotes:\nThe lack of certificate revocation was flagged up as a high risk issue in the recent\nKubernetes security audit. Without this feature, client certificate authentication is not\nsuitable for end users.\n",
        "ref": "",
        "impact": "External mechanisms for authentication generally require additional software to be\ndeployed.\n"
    },
    "3.2 Logging": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "3.2.1 Ensure that a minimal audit policy is created (Manual)": {
        "audit": "Run the following command on one of the cluster master nodes:\nps -ef | grep kube-apiserver\nVerify that the --audit-policy-file is set. Review the contents of the file specified and\nensure that it contains a valid audit policy.\nbe taken to avoid generating too large volumes of log information as this could impact the\navailable of the cluster nodes.\n",
        "remediation": "Create an audit policy file for your cluster.\n",
        "default": "Unless the --audit-policy-file flag is specified, no auditing will be carried out.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/\n",
        "impact": ""
    },
    "3.2.2 Ensure that the audit policy covers key security concerns (Manual)": {
        "audit": "Review the audit policy provided for the cluster and ensure that it covers at least the\nfollowing areas :â¢\nâ¢\nâ¢\nAccess to Secrets managed by the cluster. Care should be taken to only log Metadata\nfor requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of\nlogging sensitive data.\nModification of pod and deployment objects.\nUse of pods/exec, pods/portforward, pods/proxy and services/proxy.\nFor most requests, minimally logging at the Metadata level is recommended (the most basic\nlevel of logging).\n",
        "remediation": "Consider modification of the audit policy in use on the cluster to include these items, at a\nminimum.\n",
        "default": "By default Kubernetes clusters do not log audit information.\n",
        "ref": "1. https://github.com/k8scop/k8s-securitydashboard/blob/master/configs/kubernetes/adv-audit.yaml\n2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy\n3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/a\nudit-policy.yaml\n4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configur\ne-helper.sh#L735\n",
        "impact": "Increasing audit logging will consume resources on the nodes or other log destination.\n"
    },
    "4.1 Worker Node Configuration Files": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "4.1.1 Ensure that the kubelet service file permissions are set to 644 or": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet service config file.\nPlease set $kubelet_service_config= based on the file location on your system\nfor example:\nexport kubelet_service_config=/etc/systemd/system/kubelet.service.d/10kubeadm.conf\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchmod 755 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n",
        "default": "By default, the kubelet service file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44joining-your-nodes\n3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in\n",
        "impact": "None\n"
    },
    "4.1.2 Ensure that the kubelet service file ownership is set to root:root": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet service config file.\nPlease set $kubelet_service_config= based on the file location on your system\nfor example:\nexport kubelet_service_config=/etc/systemd/system/kubelet.service.d/10kubeadm.conf\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchown root:root /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n",
        "default": "By default, kubelet service file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44joining-your-nodes\n3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in\n",
        "impact": "None\n"
    },
    "4.1.3 If proxy kubeconfig file exists ensure permissions are set to 644 or": {
        "audit": "Find the kubeconfig file being used by kube-proxy by running the following command:\nps -ef | grep kube-proxy\nIf kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter.\nTo perform the audit:\nRun the below command (based on the file location on your system) on the each\nworker node. For example,\nstat -c %a\nVerify that a file is specified and it exists with permissions are `644` or\nmore restrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchmod 644 <proxy kubeconfig file>\n",
        "default": "By default, proxy file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-proxy/\n",
        "impact": "None\n"
    },
    "4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root": {
        "audit": "Find the kubeconfig file being used by kube-proxy by running the following command:\nps -ef | grep kube-proxy\nIf kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter.\nTo perform the audit:\nRun the below command (based on the file location on your system) on the each\nworker node. For example,\nstat -c %a\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchown root:root <proxy kubeconfig file>\n",
        "default": "By default, proxy file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kube-proxy/\n",
        "impact": "None\n"
    },
    "4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet config file.\nPlease set $kubelet_config= based on the file location on your system\nfor example:\nexport kubelet_config=/etc/kubernetes/kubelet.conf\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nVerify that the ownership is set to root:root.Verify that the permissions are 644 or more\nrestrictive.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchmod 644 /etc/kubernetes/kubelet.conf\n",
        "default": "By default, kubelet.conf file has permissions of 640.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n",
        "impact": "None\n"
    },
    "4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet config file.\nPlease set $kubelet_config= based on the file location on your system\nfor example:\nexport kubelet_config=/etc/kubernetes/kubelet.conf\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchown root:root /etc/kubernetes/kubelet.conf\n",
        "default": "By default, kubelet.conf file ownership is set to root:root.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n",
        "impact": "None\n"
    },
    "4.1.7 Ensure that the certificate authorities file permissions are set to": {
        "audit": "Run the following command:\nps -ef | grep kubelet\nFind the file specified by the --client-ca-file argument.\nRun the following command:\nstat -c %a <filename>\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the following command to modify the file permissions of the --client-ca-file\nchmod 644 <filename>\n",
        "default": "By default no --client-ca-file is specified.\n",
        "ref": "1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs\n",
        "impact": "None\n"
    },
    "4.1.8 Ensure that the client certificate authorities file ownership is set to": {
        "audit": "Run the following command:\nps -ef | grep kubelet\nFind the file specified by the --client-ca-file argument.\nRun the following command:\nstat -c %U:%G <filename>\nVerify that the ownership is set to root:root.\n",
        "remediation": "Run the following command to modify the ownership of the --client-ca-file.\nchown root:root <filename>\n",
        "default": "By default no --client-ca-file is specified.\n",
        "ref": "1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs\n",
        "impact": "None\n"
    },
    "4.1.9 Ensure that the kubelet --config configuration file has permissions": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet config yaml file.\nPlease set $kubelet_config_yaml= based on the file location on your system\nfor example:\nexport kubelet_config_yaml=/var/lib/kubelet/config.yaml\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /var/lib/kubelet/config.yaml\nVerify that the permissions are 644 or more restrictive.\n",
        "remediation": "Run the following command (using the config file location identied in the Audit step)\nchmod 644 /var/lib/kubelet/config.yaml\n",
        "default": "By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of\n644.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\n",
        "impact": "None\n"
    },
    "4.1.10 Ensure that the kubelet --config configuration file ownership is": {
        "audit": "Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the / of\nthe kubelet config yaml file.\nPlease set $kubelet_config_yaml= based on the file location on your system\nfor example:\nexport kubelet_config_yaml=/var/lib/kubelet/config.yaml\nTo perform the audit manually:\nRun the below command (based on the file location on your system) on the each worker\nnode. For example,\nstat -c %a /var/lib/kubelet/config.yaml\n```Verify that the ownership is set to `root:root`.\n",
        "remediation": "Run the following command (using the config file location identied in the Audit step)\nchown root:root /etc/kubernetes/kubelet.conf\n",
        "default": "By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by\nroot:root.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\n",
        "impact": "None\n"
    },
    "4.2 Kubelet": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "4.2.1 Ensure that the --anonymous-auth argument is set to false": {
        "audit": "If using a Kubelet configuration file, check that there is an entry for authentication:\nanonymous: enabled set to false.\nRun the following command on each node:\nps -ef | grep kubelet\nVerify that the --anonymous-auth argument is set to false.\nThis executable argument may be omitted, provided there is a corresponding entry set to\nfalse in the Kubelet config file.\n",
        "remediation": "If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to\nfalse.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--anonymous-auth=false\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, anonymous access is enabled.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubeletauthentication\n",
        "impact": "Anonymous requests will be rejected.\n"
    },
    "4.2.2 Ensure that the --authorization-mode argument is not set to": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nIf the --authorization-mode argument is present check that it is not set to AlwaysAllow. If\nit is not present check that there is a Kubelet config file specified by --config, and that file\nsets authorization: mode to something other than AlwaysAllow.\nIt is also possible to review the running configuration of a Kubelet via the /configz\nendpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate\ncredentials will provide details of the Kubelet's configuration.\n",
        "remediation": "If using a Kubelet config file, edit the file to set authorization: mode to Webhook.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--authorization-mode=Webhook\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --authorization-mode argument is set to AlwaysAllow.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubeletauthentication\n",
        "impact": "Unauthorized requests will be denied.\n"
    },
    "4.2.3 Ensure that the --client-ca-file argument is set as appropriate": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --client-ca-file argument exists and is set to the location of the client\ncertificate authority file.\nIf the --client-ca-file argument is not present, check that there is a Kubelet config file\nspecified by --config, and that the file sets authentication: x509: clientCAFile to the\nlocation of the client certificate authority file.\n",
        "remediation": "If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to\nthe location of the client CA file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--client-ca-file=<path/to/client-ca-file>\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --client-ca-file argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubeletauthentication-authorization/\n",
        "impact": "You require TLS to be configured on apiserver as well as kubelets.\n"
    },
    "4.2.4 Verify that the --read-only-port argument is set to 0 (Manual)": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --read-only-port argument exists and is set to 0.\nIf the --read-only-port argument is not present, check that there is a Kubelet config file\nspecified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.\n",
        "remediation": "If using a Kubelet config file, edit the file to set readOnlyPort to 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--read-only-port=0\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --read-only-port is set to 10255/TCP. However, if a config file is specified by -config the default value for readOnlyPort is 0.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n",
        "impact": "Removal of the read-only port will require that any service which made use of it will need\nto be re-configured to use the main Kubelet API.\n"
    },
    "4.2.5 Ensure that the --streaming-connection-idle-timeout argument is": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --streaming-connection-idle-timeout argument is not set to 0.\nIf the argument is not present, and there is a Kubelet config file specified by --config,\ncheck that it does not set streamingConnectionIdleTimeout to 0.\n",
        "remediation": "If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a\nvalue other than 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--streaming-connection-idle-timeout=5m\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --streaming-connection-idle-timeout is set to 4 hours.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://github.com/kubernetes/kubernetes/pull/18552\n",
        "impact": "Long-lived connections could be interrupted.\n"
    },
    "4.2.6 Ensure that the --protect-kernel-defaults argument is set to true": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --protect-kernel-defaults argument is set to true.\nIf the --protect-kernel-defaults argument is not present, check that there is a Kubelet\nconfig file specified by --config, and that the file sets protectKernelDefaults to true.\n",
        "remediation": "If using a Kubelet config file, edit the file to set protectKernelDefaults: true.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--protect-kernel-defaults=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --protect-kernel-defaults is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n",
        "impact": "You would have to re-tune kernel parameters to match kubelet parameters.\n"
    },
    "4.2.7 Ensure that the --make-iptables-util-chains argument is set to true": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that if the --make-iptables-util-chains argument exists then it is set to true.\nIf the --make-iptables-util-chains argument does not exist, and there is a Kubelet config\nfile specified by --config, verify that the file does not set makeIPTablesUtilChains to\nfalse.\n",
        "remediation": "If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove the --make-iptables-util-chains argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --make-iptables-util-chains argument is set to true.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n",
        "impact": "Kubelet would manage the iptables on the system and keep it in sync. If you are using any\nother iptables management solution, then there might be some conflicts.\n"
    },
    "4.2.8 Ensure that the --hostname-override argument is not set (Manual)": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that --hostname-override argument does not exist.\nNote This setting is not configurable via the Kubelet config file.\n",
        "remediation": "Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and remove the --hostname-override argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --hostname-override argument is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://github.com/kubernetes/kubernetes/issues/22063\n",
        "impact": "Some cloud providers may require this flag to ensure that hostname matches names issued\nby the cloud provider. In these environments, this recommendation should not apply.\n"
    },
    "4.2.9 Ensure that the --event-qps argument is set to 0 or a level which": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nReview the value set for the --event-qps argument and determine whether this has been\nset to an appropriate level for the cluster. The value of 0 can be used to ensure that all\nevents are captured.\nIf the --event-qps argument does not exist, check that there is a Kubelet config file\nspecified by --config and review the value in this location.\n",
        "remediation": "If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, --event-qps argument is set to 5.\n",
        "ref": "1. https://kubernetes.io/docs/admin/kubelet/\n2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubel\netconfig/v1beta1/types.go\n",
        "impact": "Setting this parameter to 0 could result in a denial of service condition due to excessive\nevents being created. The cluster's event processing and storage systems should be scaled\nto handle expected event loads.\n"
    },
    "4.2.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as\nappropriate.\nIf these arguments are not present, check that there is a Kubelet config specified by --config\nand that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.\n",
        "remediation": "If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate\nfile to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the\ncorresponding private key file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set\nthe below parameters in KUBELET_CERTIFICATE_ARGS variable.\n--tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file>\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "4.2.11 Ensure that the --rotate-certificates argument is not set to false": {
        "audit": "Run the following command on each node:\nps -ef | grep kubelet\nVerify that the --rotate-certificates argument is not present, or is set to true.\nIf the --rotate-certificates argument is not present, verify that if there is a Kubelet\nconfig file specified by --config, that file does not contain rotateCertificates: false.\n",
        "remediation": "If using a Kubelet config file, edit the file to add the line rotateCertificates: true or\nremove it altogether to use the default value.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove --rotate-certificates=false argument from the KUBELET_CERTIFICATE_ARGS\nvariable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, kubelet client certificate rotation is enabled.\n",
        "ref": "1. https://github.com/kubernetes/kubernetes/pull/41912\n2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tlsbootstrapping/#kubelet-configuration\n3. https://kubernetes.io/docs/imported/release/notes/\n4. https://kubernetes.io/docs/reference/command-line-tools-reference/featuregates/\n",
        "impact": "None\n"
    },
    "4.2.12 Verify that the RotateKubeletServerCertificate argument is set to": {
        "audit": "Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotateserver-certificates parameter is set on kubelet\nRun the following command on each node:\nps -ef | grep kubelet\nVerify that RotateKubeletServerCertificate argument exists and is set to true.\n",
        "remediation": "Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable.\n--feature-gates=RotateKubeletServerCertificate=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default, kubelet server certificate rotation is disabled.\n",
        "ref": "1. https://github.com/kubernetes/kubernetes/pull/45059\n2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubeletconfiguration\n",
        "impact": "None\n"
    },
    "4.2.13 Ensure that the Kubelet only makes use of Strong Cryptographic": {
        "audit": "The set of cryptographic ciphers currently considered secure is the following:\nâ¢\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\nâ¢\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\nâ¢\nTLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\nâ¢\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nâ¢\nTLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\nâ¢\nTLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\nâ¢\nTLS_RSA_WITH_AES_256_GCM_SHA384\nâ¢\nTLS_RSA_WITH_AES_128_GCM_SHA256\nRun the following command on each node:\nps -ef | grep kubelet\nIf the --tls-cipher-suites argument is present, ensure it only contains values included in\nthis set.\nIf it is not present check that there is a Kubelet config file specified by --config, and that\nfile sets TLSCipherSuites: to only include values from this set.\n",
        "remediation": "If using a Kubelet config file, edit the file to set TLSCipherSuites: to\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 or to a subset\nof these values.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the --tls-cipher-suites parameter as follows, or to a subset of these values.\n--tls-ciphersuites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM\n_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM\n_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM\n_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
        "default": "By default the Kubernetes API server supports a wide range of TLS ciphers\nNotes:\nThe list chosen above should be fine for modern clients. It's essentially the list from the\nMozilla \"Modern cipher\" option with the ciphersuites supporting CBC mode removed, as\nCBC has traditionally had a lot of issues\n",
        "ref": "",
        "impact": "Kubelet clients that cannot support modern cryptographic ciphers will not be able to make\nconnections to the Kubelet API.\n"
    },
    "5.1 RBAC and Service Accounts": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.1.1 Ensure that the cluster-admin role is only used where required": {
        "audit": "Obtain a list of the principals who have access to the cluster-admin role by reviewing the\nclusterrolebinding output for each role binding that has access to the cluster-admin\nrole.\nkubectl get clusterrolebindings -o=customcolumns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name\nReview each principal listed and ensure that cluster-admin privilege is required for it.\n",
        "remediation": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they\nneed this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role :\nkubectl delete clusterrolebinding [name]\n",
        "default": "By default a single clusterrolebinding called cluster-admin is provided with the\nsystem:masters group as its principal.\n",
        "ref": "1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles\n",
        "impact": "Care should be taken before removing any clusterrolebindings from the environment to\nensure they were not required for operation of the cluster. Specifically, modifications\nshould not be made to clusterrolebindings with the system: prefix as they are required\nfor the operation of system components.\n"
    },
    "5.1.2 Minimize access to secrets (Manual)": {
        "audit": "Review the users who have get, list or watch access to secrets objects in the Kubernetes\nAPI.\n",
        "remediation": "Where possible, remove get, list and watch access to secret objects in the cluster.\n",
        "default": "By default in a kubeadm cluster the following list of principals have get privileges on\nsecret objects\nCLUSTERROLEBINDING\nTYPE\nSA-NAMESPACE\nSUBJECT\ncluster-admin\nGroup\nsystem:masters\nsystem:controller:clusterrole-aggregation-controller\naggregation-controller ServiceAccount kube-system\nclusterrole-\nsystem:controller:expand-controller\nServiceAccount kube-system\nexpand-controller\nsystem:controller:generic-garbage-collector\ncollector\nServiceAccount kube-system\ngeneric-garbage-\nsystem:controller:namespace-controller\nServiceAccount kube-system\nnamespace-controller\nsystem:controller:persistent-volume-binder\nbinder\nServiceAccount kube-system\npersistent-volume-\nsystem:kube-controller-manager\nmanager\nUser\nsystem:kube-controller-\n",
        "ref": "",
        "impact": "Care should be taken not to remove access to secrets to system components which require\nthis for their operation\n"
    },
    "5.1.3 Minimize wildcard use in Roles and ClusterRoles (Manual)": {
        "audit": "Retrieve the roles defined across each namespaces in the cluster and review for wildcards\nkubectl get roles --all-namespaces -o yaml\nRetrieve the cluster roles defined in the cluster and review for wildcards\nkubectl get clusterroles -o yaml\n",
        "remediation": "Where possible replace any use of wildcards in clusterroles and roles with specific objects\nor actions.\n",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.1.4 Minimize access to create pods (Manual)": {
        "audit": "Review the users who have create access to pod objects in the Kubernetes API.\n",
        "remediation": "Where possible, remove create access to pod objects in the cluster.\n",
        "default": "By default in a kubeadm cluster the following list of principals have create privileges on\npod objects\nCLUSTERROLEBINDING\nTYPE\nSA-NAMESPACE\nSUBJECT\ncluster-admin\nGroup\nsystem:masters\nsystem:controller:clusterrole-aggregation-controller\naggregation-controller ServiceAccount kube-system\nclusterrole-\nsystem:controller:daemon-set-controller\nServiceAccount kube-system\ndaemon-set-controller\nsystem:controller:job-controller\nServiceAccount kube-system\njob-controller\nsystem:controller:persistent-volume-binder\nbinder\nServiceAccount kube-system\npersistent-volume-\nsystem:controller:replicaset-controller\nServiceAccount kube-system\nreplicaset-controller\nsystem:controller:replication-controller\nServiceAccount kube-system\nreplication-controller\nsystem:controller:statefulset-controller\nServiceAccount kube-system\nstatefulset-controller\n",
        "ref": "",
        "impact": "Care should be taken not to remove access to pods to system components which require\nthis for their operation\n"
    },
    "5.1.5 Ensure that default service accounts are not actively used.": {
        "audit": "For each namespace in the cluster, review the rights assigned to the default service account\nand ensure that it has no roles or cluster roles bound to it apart from the defaults.\nAdditionally ensure that the automountServiceAccountToken: false setting is in place for\neach default service account.\n",
        "remediation": "Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\nautomountServiceAccountToken: false\n",
        "default": "By default the default service account allows for its service account token to be mounted\nin pods in its namespace.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-serviceaccount/\n",
        "impact": "All workloads which require access to the Kubernetes API will require an explicit service\naccount to be created.\n"
    },
    "5.1.6 Ensure that Service Account Tokens are only mounted where": {
        "audit": "Review pod and service account objects in the cluster and ensure that the option below is\nset, unless the resource explicitly requires this access.\nautomountServiceAccountToken: false\n",
        "remediation": "Modify the definition of pods and service accounts which do not need to mount service\naccount tokens to disable it.\n",
        "default": "By default, all pods get a service account token mounted in them.\n",
        "ref": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-serviceaccount/\n",
        "impact": "Pods mounted without service account tokens will not be able to communicate with the API\nserver, except where the resource is available to unauthenticated principals.\n"
    },
    "5.2 Pod Security Policies": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.2.1 Minimize the admission of privileged containers (Manual)": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether privileged is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.privileged}'\nVerify that there is at least one PSP which does not return true.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.privileged field is omitted or set to false.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n",
        "impact": "Pods defined with spec.containers[].securityContext.privileged: true will not be\npermitted.\n"
    },
    "5.2.2 Minimize the admission of containers wishing to share the host": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether privileged is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.hostPID}'\nVerify that there is at least one PSP which does not return true.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostPID field is omitted or set to false.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy\n",
        "impact": "Pods defined with spec.hostPID: true will not be permitted unless they are run under a\nspecific PSP.\n"
    },
    "5.2.3 Minimize the admission of containers wishing to share the host": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether privileged is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.hostIPC}'\nVerify that there is at least one PSP which does not return true.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostIPC field is omitted or set to false.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy\n",
        "impact": "Pods defined with spec.hostIPC: true will not be permitted unless they are run under a\nspecific PSP.\n"
    },
    "5.2.4 Minimize the admission of containers wishing to share the host": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether privileged is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.hostNetwork}'\nVerify that there is at least one PSP which does not return true.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostNetwork field is omitted or set to false.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy\n",
        "impact": "Pods defined with spec.hostNetwork: true will not be permitted unless they are run\nunder a specific PSP.\n"
    },
    "5.2.5 Minimize the admission of containers with": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether privileged is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.allowPrivilegeEscalation}'\nVerify that there is at least one PSP which does not return true.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.allowPrivilegeEscalation field is omitted or set to false.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy\n",
        "impact": "Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless\nthey are run under a specific PSP.\n"
    },
    "5.2.6 Minimize the admission of root containers (Manual)": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether running containers as root is enabled:\nkubectl get psp <name> -o=jsonpath='{.spec.runAsUser.rule}'\nVerify that there is at least one PSP which returns MustRunAsNonRoot or MustRunAs with the\nrange of UIDs not including 0.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.runAsUser.rule is set to either MustRunAsNonRoot or MustRunAs with the range of\nUIDs not including 0.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n",
        "impact": "Pods with containers which run as the root user will not be permitted.\n"
    },
    "5.2.7 Minimize the admission of containers with the NET_RAW": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether NET_RAW is disabled:\nkubectl get psp <name> -o=jsonpath='{.spec.requiredDropCapabilities}'\nVerify that there is at least one PSP which returns NET_RAW or ALL.\n",
        "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.requiredDropCapabilities is set to include either NET_RAW or ALL.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-andunprivileged-linux-containers/\n",
        "impact": "Pods with containers which run with the NET_RAW capability will not be permitted.\n"
    },
    "5.2.8 Minimize the admission of containers with added capabilities": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nVerify that there are no PSPs present which have allowedCapabilities set to anything\nother than an empty array.\n",
        "remediation": "Ensure that allowedCapabilities is not present in PSPs for the cluster unless it is set to an\nempty array.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-andunprivileged-linux-containers/\n",
        "impact": "Pods with containers which require capabilities outwith the default set will not be\npermitted.\n"
    },
    "5.2.9 Minimize the admission of containers with capabilities assigned": {
        "audit": "Get the set of PSPs with the following command:\nkubectl get psp\nFor each PSP, check whether capabilities have been forbidden:\nkubectl get psp <name> -o=jsonpath='{.spec.requiredDropCapabilities}'\n",
        "remediation": "Review the use of capabilites in applications runnning on your cluster. Where a namespace\ncontains applicaions which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.\n",
        "default": "By default, PodSecurityPolicies are not defined.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-podsecurity-policies\n2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-andunprivileged-linux-containers/\n",
        "impact": "Pods with containers require capabilities to operate will not be permitted.\n"
    },
    "5.3 Network Policies and CNI": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.3.1 Ensure that the CNI in use supports Network Policies (Manual)": {
        "audit": "Review the documentation of CNI plugin in use by the cluster, and confirm that it supports\nIngress and Egress network policies.\n",
        "remediation": "If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic in\nthe Kubernetes cluster.\n",
        "default": "This will depend on the CNI plugin in use.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storagenet/network-plugins/\nNotes:\nOne example here is Flannel (https://github.com/coreos/flannel) which does not support\nNetwork policy unless Calico is also in use.\n",
        "impact": "None\n"
    },
    "5.3.2 Ensure that all Namespaces have Network Policies defined": {
        "audit": "Run the below command and review the NetworkPolicy objects created in the cluster.\nkubectl --all-namespaces get networkpolicy\nEnsure that each namespace defined in the cluster has at least one Network Policy.\n",
        "remediation": "Follow the documentation and create NetworkPolicy objects as you need them.\n",
        "default": "By default, network policies are not created.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/\n2. https://octetz.com/posts/k8s-network-policy-apis\n3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-networkpolicy/\n",
        "impact": "Once network policies are in use within a given namespace, traffic not explicitly allowed by\na network policy will be denied. As such it is important to ensure that, when introducing\nnetwork policies, legitimate traffic is not blocked.\n"
    },
    "5.4 Secrets Management": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.4.1 Prefer using secrets as files over secrets as environment variables": {
        "audit": "Run the following command to find references to objects which use environment variables\ndefined from secrets.\nkubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind}\n{.metadata.name} {\"\n\"}{end}' -A\n",
        "remediation": "If possible, rewrite application code to read secrets from mounted secret files, rather than\nfrom environment variables.\n",
        "default": "By default, secrets are not defined\n",
        "ref": "1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets\nNotes:\nMounting secrets as volumes has the additional benefit that secret values can be updated\nwithout restarting the pod\n",
        "impact": "Application code which expects to read secrets in the form of environment variables would\nneed modification\n"
    },
    "5.4.2 Consider external secret storage (Manual)": {
        "audit": "Review your secrets management implementation.\n",
        "remediation": "Refer to the secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n",
        "default": "By default, no external secret management is configured.\n",
        "ref": "",
        "impact": "None\n"
    },
    "5.5 Extensible Admission Control": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.5.1 Configure Image Provenance using ImagePolicyWebhook": {
        "audit": "Review the pod definitions in your cluster and verify that image provenance is configured\nas appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and setup image provenance.\n",
        "default": "By default, image provenance is not set.\n",
        "ref": "1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook\n2. https://github.com/kubernetes/community/blob/master/contributors/designproposals/image-provenance.md\n3. https://hub.docker.com/r/dnurmi/anchore-toolbox/\n4. https://github.com/kubernetes/kubernetes/issues/22888\n",
        "impact": "You need to regularly maintain your provenance configuration based on container image\nupdates.\n"
    },
    "5.7 General Policies": {
        "audit": "",
        "remediation": "",
        "default": "",
        "ref": "",
        "impact": ""
    },
    "5.7.1 Create administrative boundaries between resources using": {
        "audit": "Run the below command and review the namespaces created in the cluster.\nkubectl get namespaces\nEnsure that these namespaces are the ones you need and are adequately administered as\nper your requirements.\n",
        "remediation": "Follow the documentation and create namespaces for objects in your deployment as you\nneed them.\n",
        "default": "By default, Kubernetes starts with two initial namespaces:\n1. default - The default namespace for objects with no other namespace\n2. kube-system - The namespace for objects created by the Kubernetes system\n",
        "ref": "1. https://kubernetes.io/docs/concepts/overview/working-withobjects/namespaces/\n2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetesdeployment.html\n",
        "impact": "You need to switch between namespaces for administration.\n"
    },
    "5.7.2 Ensure that the seccomp profile is set to docker/default in your": {
        "audit": "Review the pod definitions in your cluster. It should create a line as below:\nannotations:\nseccomp.security.alpha.kubernetes.io/pod: docker/default\n",
        "remediation": "Seccomp is an alpha feature currently. By default, all alpha features are disabled. So, you\nwould need to enable alpha features in the apiserver by passing \"--featuregates=AllAlpha=true\" argument.\nEdit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS\nparameter to \"--feature-gates=AllAlpha=true\"\nKUBE_API_ARGS=\"--feature-gates=AllAlpha=true\"\nBased on your system, restart the kube-apiserver service. For example:\nsystemctl restart kube-apiserver.service\nUse annotations to enable the docker/default seccomp profile in your pod definitions. An\nexample is as below:\napiVersion: v1\nkind: Pod\nmetadata:\nname: trustworthy-pod\nannotations:\nseccomp.security.alpha.kubernetes.io/pod: docker/default\nspec:\ncontainers:\n- name: trustworthy-container\nimage: sotrustworthy:latest\n",
        "default": "By default, seccomp profile is set to unconfined which means that no seccomp profiles are\nenabled.\n",
        "ref": "1. https://github.com/kubernetes/kubernetes/issues/39845\n2. https://github.com/kubernetes/kubernetes/pull/21790\n3. https://github.com/kubernetes/community/blob/master/contributors/designproposals/seccomp.md#examples\n4. https://docs.docker.com/engine/security/seccomp/\n",
        "impact": "If the docker/default seccomp profile is too restrictive for you, you would have to\ncreate/manage your own seccomp profiles. Also, you need to enable all alpha features for\nthis to work. There is no individual switch to turn on this feature.\n"
    },
    "5.7.3 Apply Security Context to Your Pods and Containers (Manual)": {
        "audit": "Review the pod definitions in your cluster and verify that you have security contexts\ndefined as appropriate.\n",
        "remediation": "Follow the Kubernetes documentation and apply security contexts to your pods. For a\nsuggested list of security contexts, you may refer to the CIS Security Benchmark for Docker\nContainers.\n",
        "default": "By default, no security contexts are automatically applied to pods.\n",
        "ref": "1. https://kubernetes.io/docs/concepts/policy/security-context/\n2. https://learn.cisecurity.org/benchmarks\n",
        "impact": "If you incorrectly apply security contexts, you may have trouble running the pods.\n"
    },
    "5.7.4 The default namespace should not be used (Manual)": {
        "audit": "Run this command to list objects in default namespace\nkubectl get all\nThe only entries there should be system managed resources such as the kubernetes service\n",
        "remediation": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n",
        "default": "Unless a namespace is specific on object creation, the default namespace will be used\n",
        "ref": "",
        "impact": "None\n"
    }
}